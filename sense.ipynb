{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\Desktop\\Caduceus\\Caduceus\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.config import Settings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file if needed\n",
    "# load_dotenv()\n",
    "\n",
    "# Define the folder for storing the database\n",
    "PERSIST_DIRECTORY = 'db2'  # Set your desired directory\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    chroma_db_impl='duckdb+parquet',\n",
    "    persist_directory=PERSIST_DIRECTORY,\n",
    "    anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "# Define the source directory where input documents are located\n",
    "SOURCE_DIRECTORY = 'input'  # Set the path to your input directory\n",
    "\n",
    "# Define the embeddings model name\n",
    "EMBEDDINGS_MODEL_NAME = 'bert-base-nli-stsb-mean-tokens'  # Change to your desired model\n",
    "\n",
    "# Define constants for text splitting\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Define the directory where Sentence Transformer models will be stored\n",
    "MODEL_DIRECTORY = 'models'  # Update this path as per your requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".pdf\": (PyMuPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}\n",
    "\n",
    "def get_embed_model(model_name, model_directory):\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(model_directory):\n",
    "        os.makedirs(model_directory)\n",
    "\n",
    "    model_path = os.path.join(model_directory, model_name)\n",
    "\n",
    "    # Check if model files exist in the directory\n",
    "    if not os.path.exists(model_path):\n",
    "        # If model files don't exist, download them and save\n",
    "        model = SentenceTransformer(model_name)\n",
    "        model.save(model_path)\n",
    "\n",
    "    return model_path\n",
    "\n",
    "def load_single_document(file_path: str) -> list[Document]:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: list[str] = []) -> list[Document]:\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
    "        )\n",
    "\n",
    "    filtered_files = [\n",
    "        file_path for file_path in all_files if file_path not in ignored_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_documents(ignored_files: list[str] = []) -> list[Document]:\n",
    "    print(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
    "    documents = load_documents(SOURCE_DIRECTORY, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Loaded {len(documents)} new documents from {SOURCE_DIRECTORY}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(\n",
    "        f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore\n",
      "Loading documents from input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents:   0%|                              | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model_path = get_embed_model(EMBEDDINGS_MODEL_NAME, MODEL_DIRECTORY)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_path)\n",
    "\n",
    "    if os.path.exists(PERSIST_DIRECTORY):\n",
    "        print(\"Removing existing vectorstore\")\n",
    "        os.system(f\"rm -r {PERSIST_DIRECTORY}\")\n",
    "\n",
    "    print(\"Creating new vectorstore\")\n",
    "    texts = process_documents()\n",
    "    if not texts:\n",
    "        print(\"No texts to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Creating embeddings for {len(texts)} chunks of text\")\n",
    "    db = Chroma.from_documents(\n",
    "        texts, embeddings, persist_directory=PERSIST_DIRECTORY, client_settings=CHROMA_SETTINGS)\n",
    "    db.persist()\n",
    "    db = None\n",
    "\n",
    "    print(f\"Ingestion complete!\")\n",
    "\n",
    "# Run the main function to process the documents\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Caduceus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
